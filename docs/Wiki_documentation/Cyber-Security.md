# Strategy

We have assigned a security officer (Junbo Liang) to handle potential security issues in the system. Before actually implementing these features, the security officer needs to conduct an analysis based on the guidelines introduced in class.

We listed all the required external open source dependencies. We used CVE to find the vulnerabilities and then used NVD for analysis. After that, we used CWE to guide us to avoid making the same coding mistakes and prioritized them based on CVSS scores.
 
# Secure Coding Practices

* SQL Injection: Not applicable. We do not use a database to store user information. According to the client's requirements, our project is a key function in their system, so they will be responsible for handling their user information. We only need to support that a few authorized programmers to access our system.

* URL Manipulation: In our system, Dify deployed on the MRC virtual machine, users cannot navigate to other users' models by modifying the URL. They need to log in to their account correctly and then navigate to the content in the GUI.

* XSS: On the frontend, we convert everything the user enters into a string and then process the POST request and send to the backend. Also, the user input is part of the payload we send via HTML and our backend also has a logic that extracts the input and treats it as plain text.

* XSS_cookie: We don't have cookie in our system.

# Data Protection

The frontend only requests necessary fields to minimize data exposure, and the backend does not return any user-related information except the sentences generated by the backend model. And our system is designed not to store any information related to users.

# Authentication

Our backend is deployed on a VM in the MRC which has limited access and only a few devops engineers can access it. They need to apply to join the project first, submit their public key to the MRC administrator, and then use ssh and their private key to access the system. These authorized private keys are not allowed to be shared with anyone and are stored locally on one's own device. Through this strict process, any unauthorized users are prevented from damaging the environment. Logging into a Dify account requires a username and password, which are stored in the VM's root directory so that only authorized people know them.

# Secure Third-party Integrations

* We listed all third-party libraries and carefully checked them for potential vulnerabilities and evaluated them carefully. We only installed these external libraries from trusted sources and installed them using pip command.

* When using LLM's API, make sure to only use those well-known platforms and LLM's inferences, not LLM itself, or use some potentially risky LLMs, such as Deepseek. Don't hardcode APIs, store them in environment variables instead.

* We do not use external unverified databases, only verified databases and embed them into our local backend knowledge base. We do not use external databases to store any form of data, so there is no risk of data leakage.

* We deployed the backend in a virtual machine at MRC and which has set strict access permissions. We do not make public and floating IPs and ports accessible, only the users within the Unimelb network or using the Unimelb VPN can access, to prevent any form of external unauthorized access.

* Our backend (Dify) is deployed on a virtual machine that runs independently of the public Dify website. This means all user data stays local on our virtual machine, and even the company behind Dify cannot access it.

# Security Measures in Development

During the development process, the security owner tests the system, uncovering potential vulnerabilities and bugs to the developers to ensure that these issues are addressed before a single feature is added to the system. We use ChatGPT code safety auto-reviewer to check our code before merging any form of code into the master branch.

# Risk Evaluation
|Risk|Trigger|Likelihood|Impact|Plan|
|------|------|------|------|------|
|SQL injection|User input is passed directly to SQL query|High|Low (We don't really have DB to be breached)|Both front-end and backend set the logic to process the user input|
|Hardcoded API key|Developer pushes code with secrets to GitHub|Low|Low (They cannot access the system via the API and also require a key pair for ssh connections)|We store the API in .env file|
|Lack of authentication|Public-facing admin|Low|High (unauthorized access)|We store the admin account information in the VM where only authorized devops engineers can find it|
|Floating IP access|Users can access the system with IP that is outside of unimelb|High|High (More potential attacks to the system)|Disable the floating IP of the virtual machine so that only the unimelb network or unimelb VPN can access it|
|Data Poisoning via RAG|RAG Retrieval + Injection|Medium|High (They could damage our backend since RAG is run from there.)|Only retrieved checked data and limit RAG to search only our local knowledge base|
|Sensitive information leaked to LLM|User feed the sensitive information through our system to the LLM|Low|Low (We using the LLM inference, those data is neither memory by the LLM nor used as training purpose)|Make sure we only use the inference of LLM, not LLM itself, or use some potentially risky LLM, such as Deepseek|
|Man-in-the-middle (MITM) via SSH tunneling|Unencrypted traffic through SSH port forwarding if not configured securely|Medium|High (They could steal our private key)|Ensure SSH port forwarding uses strong encryption|
|Inference abuse|User sends repeated or malformed inputs to overwhelm LLM or force high compute usage|High|High (System overload, high cost)|Rate limit user inputs, add max length and frequency caps to LLM queries, and monitor the usage of the API to access the LLM inference|
|Dependency vulnerability|Install them from some private image|Low|High (Hackers may leave a backdoor in it)|Use 'pip' to install them from official sources, and only use them after carefully analysis|
|Misconfiguration in cloud VM|Accidental open port|Low|High (Some ports may be used by other functions on the virtual machine)|Set up security groups in the MRC dashboard to only expose a small number of ports to users|
|Prompt injection via user input|Malicious user crafts input to manipulate LLM behavior|Medium|High (They could change the LLM and leaking the data)|Sanitize user input, use LLM guardrails or prompt shaping to restrict unsafe instructions|
|Incomplete knowledge base filtering|Local knowledge base still contains unverified data|Medium|Medium (Some unverified data might show to user and misleading them)|All the database need to be verified first, and embedding in the knowledge before using|
|LLM hallucination with authority|LLM returns fabricated but confident responses|Medium|Medium (LLM starts talking nonsence)|Properly prompt the LLM before starting a conversation|



