## Overview
This project aims to develop and deploy a scenario-based Chatbot system to help users get advice and improve their decision-making ability in different aviation situations. The system provides users with guidance, scenario exercises and other services through intelligent dialogue generation, and is widely used in various scenarios such as training and customer service. However, with the in-depth application of artificial intelligence technology, while the system improves efficiency and experience, it also inevitably brings a series of potential ethical risks. We take ethical responsibilities seriously and have established a living document that outlines key ethical considerations throughout the development of our AI system. This document is regularly reviewed and updated as the project evolves.

## Sources of ethical risks
### The ethical risks of Chatbot systems mainly come from the following aspects:

- Technical limitations: including uncertainty in model output, data training bias, untimely knowledge updates, etc.

- User behavior: such as over-reliance on AI suggestions, neglect of independent judgment, and incorrect use of high-intensity tone.

- Data and privacy management: involving user history records, storage and security protection of sensitive data.

- External attacks and system security: including security risks such as illegal intrusion into the database and leakage of sensitive data.

### principles
#### **Privacy**

- All user inputs and generated data are processed locally on our virtual machine. We do not store personal data long-term, and no third party, including the company behind Dify, can access the data.

- We clearly communicate to users that their inputs are used for real-time inference only, not for training or logging purposes.

- We avoid tracking or analytics scripts that collect user behavior data.

#### **Transparency**

- Users are informed that the system uses a large language model and retrieval-augmented generation (RAG) to simulate scenarios based on available information.
- When LLM-generated content is shown, we clarify which parts are based on retrieved knowledge and which are inferred/generated.

- We provide a simple explanation of how the system works in the user interface (UI), helping users understand the limitations of the AI.

#### **Inclusivity**

- The system is designed to accommodate diverse users, including non-native English speakers, by supporting clear, plain-language prompts and error-tolerant input handling.

- We avoid biases by curating our local knowledge base and minimizing dependence on uncontrolled public web data.

- Our team actively considers potential cultural, gender, or social biases in AI responses and tests interactions with a range of use cases.

#### **Sustainability**
- To reduce environmental impact, we deploy the system on a shared research cloud (Melbourne Research Cloud) rather than spinning up dedicated GPU infrastructure.


We have identified some ethical issues related to the project and recorded in detail the main risk type, likelihood of occurrence, potential severity, and corresponding ethical risk management measures for each issue in the table below. These contents help us systematically analyze and respond to the ethical challenges that the project may face in practical applications.

---





## Ethical Consideration Table 

| Ethical issue  | Primary risk type (Team /Tech /Other) |Principle| Likelihood of occurrence (1=low likelihood; 5=high likelihood) |Potential Severity (1=low severity; 5=high severity)| Ethical management approaches|
|----------------|-----------------------------------|----|----------------------|----------|------------------------------|
| The guidance provided by the chatbot may be incorrect, which would the user to perform incorrect operations | Tech |Transparency| 3 | 5 | Inform users in advance that the guidance provided by the chatbot is for reference only and its content may be inaccurate or inapplicable to specific cases. Encouraged user to carefully review and judge the actual situation before adopting the solution.  |
| Users may over-rely on Chatbotâ€™s suggestions and ignore independent judgment or professional advice, which can bring risks in actual critical scenarios. | Other|Transparency| 2 | 5 | Clearly remind users that Chatbot suggestions are for reference only, especially to warn users in high-risk scenarios.| 
| Since Chatbot supports the tone intensity adjustment, when the user chooses a higher intensity tone, the conversation generated by the model may appear more aggressive. This conversation style may have a negative impact or emotional distress on the user. | Tech |Transparency and Inclusivity| 2 | 3 | 1. providing warnings before users select high-intensity tone options. <br> 2. Implementing content moderation mechanisms to limit excessively aggressive outputs.| 
| The model may generate biased and discriminatory scenario simulation responses based on the training data, leading to unfair treatment of certain groups. | Tech |Inclusivity| 2 | 3 | 1. Use a larger language model to clean up biased areas in the training data. <br> 2. Provide a user feedback channel to help the team identify problems and make improvements.| 
| The system database may be attacked, and user historical chat records and other sensitive data such as training data may be illegally obtained, which may cause widespread privacy risks. | Tech |Privacy| 3 | 5 | 1. Strengthen database security protection, such as setting up firewalls, and access control. <br> 2. Establish a database access monitoring and abnormal alarm mechanism to promptly detect and respond to abnormal behavior.| 
| The model knowledge base is not updated in a timely manner, and outdated content is output, misleading users. | Tech |Sustainability| 3 | 3 | 1. Update data and knowledge base regularly. <br> 2. Mark the source of knowledge and update time. <br> 3. Encourage users to report knowledge errors.| 
